<<<<<<< HEAD
{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import keras\n",
    "\n",
    "os.getcwd()\n",
    "if '.git' not in os.listdir():\n",
    "    os.chdir('./../../../../')\n",
    "os.listdir()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, TFCamembertModel\n",
    "import tensorflow as tf\n",
    "import keras_core as keras\n",
    "\n",
    "import tensorflow_hub as hub\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T17:41:12.009746Z",
     "start_time": "2024-07-09T17:41:12.005411Z"
    }
   },
   "id": "697e5d4ae00256bd",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "params_grid = {\n",
    "    \"embedder_data\": \"almanach-camembert-base\",\n",
    "    \"dataset\": \"corrected_format_all_data.csv\"\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T17:41:12.014525Z",
     "start_time": "2024-07-09T17:41:12.011219Z"
    }
   },
   "id": "c8c732d75b962571",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_folder_src = 'ml/ml_core/data/processed/'\n",
    "data_path_src = f'{data_folder_src}{params_grid[\"dataset\"]}'\n",
    "\n",
    "vector_path = f'ml/ml_core/embedded_vector/{params_grid[\"embedder_data\"]}_{params_grid[\"dataset\"]}'\n",
    "vector_x_dst = f'{vector_path}.x.npy'\n",
    "vector_y_dst = f'{vector_path}.y.npy'\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T17:41:12.020121Z",
     "start_time": "2024-07-09T17:41:12.016765Z"
    }
   },
   "id": "3f1ffbbe1322a51b",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "3d946bd3803cbf47",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## import CamemBERT model"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"almanach/camembert-base\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T17:41:12.216164Z",
     "start_time": "2024-07-09T17:41:12.021437Z"
    }
   },
   "id": "808824cd0e8ca8f9",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 19:41:12.660710: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-09 19:41:12.901673: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-09 19:41:12.901706: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-09 19:41:12.903918: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-09 19:41:12.903951: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-09 19:41:12.903968: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-09 19:41:13.065539: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-09 19:41:13.065582: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-09 19:41:13.065589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-07-09 19:41:13.065614: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-09 19:41:13.065632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7537 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFCamembertModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing TFCamembertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFCamembertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFCamembertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCamembertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFCamembertModel.from_pretrained(\"almanach/camembert-base\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T17:41:15.093047Z",
     "start_time": "2024-07-09T17:41:12.217473Z"
    }
   },
   "id": "928679d6b202d99c",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def encode_texts(texts):\n",
    "    inputs = tokenizer(texts, return_tensors=\"tf\")\n",
    "    outputs = model(inputs)\n",
    "    outputs2 = outputs.last_hidden_state\n",
    "    \n",
    "    return outputs2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T17:41:15.097412Z",
     "start_time": "2024-07-09T17:41:15.094447Z"
    }
   },
   "id": "e1ce7e470d9036fa",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "ds = pd.read_csv(data_path_src)\n",
    "e"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T18:31:07.648690Z",
     "start_time": "2024-07-09T18:31:07.615546Z"
    }
   },
   "id": "fbf9edee5f550797",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16898/16898 [36:39<00:00,  7.68it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 35 and the array at index 1 has size 24",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m tqdm\u001B[38;5;241m.\u001B[39mpandas(desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLoading data\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      4\u001B[0m ds[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvector\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m ds[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mprogress_apply(encode_texts)\n\u001B[0;32m----> 6\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mds\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mvector\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m y \u001B[38;5;241m=\u001B[39m ds[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\n",
      "File \u001B[0;32m~/miniconda3/envs/DEV_IA/lib/python3.10/site-packages/numpy/core/shape_base.py:289\u001B[0m, in \u001B[0;36mvstack\u001B[0;34m(tup, dtype, casting)\u001B[0m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arrs, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m    288\u001B[0m     arrs \u001B[38;5;241m=\u001B[39m [arrs]\n\u001B[0;32m--> 289\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_nx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconcatenate\u001B[49m\u001B[43m(\u001B[49m\u001B[43marrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcasting\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcasting\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mValueError\u001B[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 35 and the array at index 1 has size 24"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas(desc='Loading data')\n",
    "ds['vector'] = ds['text'].progress_apply(encode_texts)\n",
    "tulaseeiua\n",
    "eiuaX = np.vstackii(ds['vector'])opéb\n",
    "y = ds['labels'].values\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T18:17:57.421627Z",
     "start_time": "2024-07-09T17:41:15.125358Z"
    }
   },
   "id": "2306f9f40ce2b4ec",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "9dec354f696f24a9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### saving embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee4f2d23f45322e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save(vector_x_dst, X)\n",
    "np.save(vector_y_dst, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
=======
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "697e5d4ae00256bd",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "is_executing": true,
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-13 17:58:04.597378: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-13 17:58:04.597727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-13 17:58:04.598540: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-13 17:58:04.711651: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import keras\n",
    "\n",
    "os.getcwd()\n",
    "if '.git' not in os.listdir():\n",
    "    os.chdir('./../../../../')\n",
    "os.listdir()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, TFCamembertModel\n",
    "import tensorflow as tf\n",
    "import keras_core as keras\n",
    "\n",
    "import tensorflow_hub as hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8c732d75b962571",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T17:41:12.014525Z",
     "start_time": "2024-07-09T17:41:12.011219Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "params_grid = {\n",
    "    \"embedder_data\": \"camembert\",\n",
    "    \"dataset\": \"corrected_format_all_data.csv\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f1ffbbe1322a51b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T17:41:12.020121Z",
     "start_time": "2024-07-09T17:41:12.016765Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data_folder_src = 'ml/ml_core/data/processed/'\n",
    "data_path_src = f'{data_folder_src}{params_grid[\"dataset\"]}'\n",
    "\n",
    "vector_path = f'ml/ml_core/embedded_vector/{params_grid[\"embedder_data\"]}_{params_grid[\"dataset\"]}'\n",
    "vector_x_dst = f'{vector_path}.x.npy'\n",
    "vector_y_dst = f'{vector_path}.y.npy'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d946bd3803cbf47",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## import CamemBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "808824cd0e8ca8f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T17:41:12.216164Z",
     "start_time": "2024-07-09T17:41:12.021437Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"almanach/camembert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "928679d6b202d99c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T17:41:15.093047Z",
     "start_time": "2024-07-09T17:41:12.217473Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-13 17:58:10.290647: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-13 17:58:10.564320: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-13 17:58:10.564355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-13 17:58:10.575680: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-13 17:58:10.575734: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-13 17:58:10.575752: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-13 17:58:10.974820: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-13 17:58:10.974890: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-13 17:58:10.974901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-07-13 17:58:10.974927: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-13 17:58:10.975028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2588 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFCamembertModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing TFCamembertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFCamembertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFCamembertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCamembertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFCamembertModel.from_pretrained(\"almanach/camembert-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1ce7e470d9036fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T17:41:15.097412Z",
     "start_time": "2024-07-09T17:41:15.094447Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def encode_texts(texts):\n",
    "    inputs = tokenizer(texts, return_tensors=\"tf\")\n",
    "    outputs = model(inputs)\n",
    "    outputs2 = outputs.last_hidden_state\n",
    "    \n",
    "    return outputs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbf9edee5f550797",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T18:31:07.648690Z",
     "start_time": "2024-07-09T18:31:07.615546Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "ds = pd.read_csv(data_path_src)\n",
    "#ds = ds[ds['source'] != \"role_playing_game\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2306f9f40ce2b4ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T18:17:57.421627Z",
     "start_time": "2024-07-09T17:41:15.125358Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data:  99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 16689/16898 [1:33:09<00:41,  4.99it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas(desc='Loading data')\n",
    "ds['vector'] = ds['text'].progress_apply(encode_texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca159cc9-3226-43ce-8aa7-ece6c53d4c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ds['vector']\n",
    "y = ds['labels'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dec354f696f24a9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### saving embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee4f2d23f45322e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "np.save(vector_x_dst, X)\n",
    "np.save(vector_y_dst, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eeba3a-48e6-45ad-ad63-5a434d6df5c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
>>>>>>> 3291550 (update models and add vit)
